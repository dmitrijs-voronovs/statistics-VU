---
title: "Assignment 1"
author: "Group 69: Dmitrijs Voronovs (2779206), Alina Boshchenko 2732782"
date: "24.02.2023"
output: 
  pdf_document: 
    latex_engine: lualatex
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

## Exercise 1

```{=html}
<!--# 
The Amsterdamsche Bos forestry wishes to estimate the total wood volume of the trees on its domain. To this end the forestry has cut a sample of 59 trees of their most prevalent types beech and oak and collected the data from the cut trees in the file treeVolume.txt Download treeVolume.txt. The volume of these trees alongside with their height and trunk diameter have been measured; these are the columns volume, height and diameter, respectively. Column type gives the tree type: Beech or Oak. The tree type, height and diameter can be measured in the field without sacrificing the tree. The forestry hypothesizes that these are predictive of the wood volume.
 -->
```
### a)

```{=html}
<!--# 
a)  Investigate whether the tree type influences volume by performing ANOVA, without taking diameter and height into account. Can a t-test be related to the above ANOVA test? Estimate the volumes for the two tree types.
 -->
```
```{r echo = T, results = 'hide'}
treeVolume = read.table("treeVolume.txt", header = T)
head(treeVolume)
```

```{r echo = F, results = 'hide'}
volumelm = lm(volume ~ factor(type), data = treeVolume)
anova(volumelm)
```

Since P value for type is 0.1736, it indicates that we fail to reject
the null hypothesis and conclude that there is no significant difference
in mean volume between the beech and oak trees.

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(volumelm))
plot(fitted(volumelm), residuals(volumelm))
par(mfrow=c(1,1))
```

After testing the data for normality, it becomes obvious that there is a
deviation of residuals from the normal distribution. As ANOVA
assumptions have been violated, p-value may not be reliable.

```{r echo = T, results = 'hide'}
t.test(volume ~ factor(type), data=treeVolume)
```

T-test shows the p-value 0.1659, indicating that we do not reject a null
hypothesis. That indicated the difference in means between volumes of
beech and oak is different.

T-test also displays estimates of mean for group beech (30.17) and oak
(35.25)

### b)

```{=html}
<!--# 
b)  Now include diameter and height as explanatory variables into the analysis. Investigate whether the influence of diameter on volume is similar for the both tree types. Do the same for the influence of height on volume. (Consider at most one (relevant) pairwise interaction per model.) Comment.
 -->
```
```{r results='hide'}
volumelm1 = lm(volume ~ height + factor(type) * diameter, data = treeVolume)
anova(volumelm1)
```

According to the p-value (0.474) of type and diameter interaction, there
is no significant difference between the influence of diameter to volume
for different tree types.

```{r}
volumelm2 = lm(volume ~ diameter + factor(type) * height, data = treeVolume)
anova(volumelm2)
```

Similarly, the p-value (0.176) of type and height interaction tells that
there is no significant difference between the influence of height to
volume for different tree types.

### c)

```{=html}
<!--# 

c)  Using the results from c), investigate how diameter, height and type influence volume. Comment. Using the resulting model, predict the volume for a tree with the (overall) average diameter and height?

 -->
```
According the anova results in **b)**, there is no need to include
interaction terms with tree type.

```{r results='hide'}
volumeFulllm = lm(volume ~ diameter + height + factor(type), data = treeVolume)
anova(volumeFulllm)
```

However, after analyzing the output of anova for the new model, it
becomes clear that type can also be excluded, as it is not significant
(p-value = 0.143).

```{r results='hide'}
volumeFulllm = lm(volume ~ diameter + height, data = treeVolume)
anova(volumeFulllm)
```

There is one more opportunity left to improve the final model by testing
the interaction of diameter and height.

```{r}
volumeFulllm = lm(volume ~ diameter * height, data = treeVolume)
anova(volumeFulllm)
```

And according to the output this time interaction is significant. Thus
let us test the anova assumptions.

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(volumeFulllm))
qqline(residuals(volumeFulllm))
plot(fitted(volumelm), residuals(volumelm))
par(mfrow=c(1,1))

```

Residuals seem to follow normal distribution, while spread in the
residuals seems to be bigger for smaller fitted values, also some points
are extreme.

Now we can predict the volume of overall average diameter and height
tree which is

```{r}
d = mean(treeVolume$diameter)
h = mean(treeVolume$height)
avgTree = data.frame(diameter=d, height=h, type="oak")

unname(predict(volumeFulllm, avgTree))

```

### d)

```{=html}
<!--# 

d)  Propose a transformation of the explanatory variables that possibly yields a better model (verify this).  (Hint: think of a natural link between the response and explanatory variables.)

 -->
```
For the natural transformation of data, let us use the formula of
cylinder volume $V = \pi*r^2*h$

```{r}
perfectVolume = pi * (treeVolume$diameter / 2)**2 * treeVolume$height
volumePerfectlm = lm(treeVolume$volume ~ perfectVolume)
anova(volumePerfectlm)
```

One way to compare model effectiveness is to compare residual standard
error.

```{r results='hide'}
summary(volumeFulllm)
summary(volumePerfectlm)
```

Residual standard error for previous model is **2.788**, while for the
more natural one is **2.284**, which indicates that it is performing
better.

## Exercise 2

```{=html}
<!--# 

The data in expensescrime.txt Download expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

 -->
```
### a)

```{=html}
<!--# 

a)  Make some graphical summaries of the data. Investigate the problem of influence points, and the problem of collinearity.

 -->
```
```{r}
expensescrime = read.table("expensescrime.txt", header = T)
pairs(expensescrime[,-1])
```

Influence points:

many paired scatter plots (i.e. expend vs bad, lawyers vs employ) have
most of the data skewed to one side and some strong outliers on the
other side.

```{r}
par(mfrow=c(1,2))
plot(expend ~ bad, data=expensescrime)
plot(lawyers ~ employ, data=expensescrime)
par(mfrow=c(1,1))
```

Collinearity:

Multiple variables are clearly collinear: bad & employ
(`cor(expensescrime$bad, expensescrime$employ)` is 0.871), bad & lawyers
(0.832), bad & pop (0.92), lawyers & employ (0.966), lawyers & pop
(0.934)

```{r}
library(ggplot2)

# number of lawyers per state
ggplot(expensescrime, aes(x = state, y = lawyers)) +
  geom_point() +
  labs(x = "State", y = "Number of Lawyers") 

#expenses per state
ggplot(expensescrime, aes(x = state, y = expend)) +
  geom_point() +
  labs(x = "State", y = "Expenses") 

#check what states are outliers in terms of expenses and lawyers 
ggplot(expensescrime, aes(x = lawyers, y = expend, label = state)) +
  geom_point() +
  geom_text(vjust = 1, hjust = 1) +
  labs(x = "Number of Lawyers", y = "Expenditures on Criminal Activities ($1000)") +
  theme_bw()

```

### b)

<!--# b)  Fit a linear regression model to the data. Use the step-up method to find the best model. Comment. -->

```{r}
summary(lm(expend ~ bad, data=expensescrime))$r.squared
summary(lm(expend ~ crime, data=expensescrime))$r.squared
summary(lm(expend ~ lawyers, data=expensescrime))$r.squared
summary(lm(expend ~ employ, data=expensescrime))$r.squared
summary(lm(expend ~ pop, data=expensescrime))$r.squared


```

The first variable to add is **employ**, as it is significant and yields
the best multiple R-squared value **0.9539745**

```{r}
summary(lm(expend ~ employ + bad, data=expensescrime))$r.squared
summary(lm(expend ~ employ + crime, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers, data=expensescrime))$r.squared
summary(lm(expend ~ employ + pop, data=expensescrime))$r.squared
```

Next one to add is **lawyers** with the R-squared value **0.9631745**

```{r}
summary(lm(expend ~ employ + lawyers + bad, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers + crime, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers + pop, data=expensescrime))$r.squared
```

Other variables upon testing showed no significance, therefore the final
model is

```{r}
model = lm(expend ~ employ + lawyers, data=expensescrime)
```

### c)

<!--# c)  Determine a 95% prediction interval for the expend using the model you preferred in b) for a (hypothetical) state with bad=50, crime=5000, lawyers=5000, employ=5000 and pop=5000. Can you improve this interval? -->

```{r}
newData = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(model, newData, interval = 'prediction')
```

In order to improve the interval we should revise the expend column. As
per definition, expend - state expenditures on criminal activities in
\$1000. Consequently, expend should always be **\>0**, thus the new
interval is **[0, 647.3504]**

### d)

<!--# Apply the LASSO method to choose the relevant variables (with default parameters as in the lecture and lambda=lambda.1se). (You will need to install the R-package glmnet, which is not included in the standard distribution of R.) Compare the resulting model with the model obtained in b). (Beware that in general a new run delivers a new model because of a new train set.) -->

Remove state column, split data into response and predictor data,
prepare training and test data.

```{r}
# columns 1 (state), 2 (expend)
x=as.matrix(expensescrime[,-1:-2])
y=expensescrime[,2]

train=sample(1:nrow(x),2/3*nrow(x))
x.train=x[train,]; y.train=y[train]
x.test=x[-train,]; y.test = y[-train]
```

Apply LASSO

```{r}
library(glmnet)
lasso.model=glmnet(x.train, y.train, alpha=1)
lasso.cv=cv.glmnet(x.train, y.train, alpha=1, type.measure="mse", nfolds=5)

plot(lasso.model, label=T, xvar="lambda")
plot(lasso.cv)
plot(lasso.cv$glmnet.fit, xvar="lambda", label=T)

lambda.1se=lasso.cv$lambda.1se;
coef(lasso.model,s=lasso.cv$lambda.1se)
lasso.pred=predict(lasso.model,s=lambda.1se,newx=as.matrix(x.test))
```

Most of the time lasso selects only lawyers, employ and pop as
predictors.

```{r}
mse.lasso=mean((y.test-lasso.pred)^2); mse.lasso

lm.model=lm(expend ~ employ + lawyers, data=expensescrime, subset=train)
y.predict.lm=predict(lm.model,newdata=expensescrime[-train,])
mse.lm=mean((y.test-y.predict.lm)^2); mse.lm
```

As results are highly dependent on the data sample, one can not claim
that one model is better than the other, however by running tests
multiple times we observed that the Lasso method was worse

## Exercise 3

### a)

<!--# a)  Study the data and give a few (>1) summaries (graphics or tables). Fit a logistic regression model (no interactions yet) to investigate the association between the survival status and the predictors PClass, Age and Sex. Interpret the results in terms of odds, comment. -->

```{r}
titanic = read.table("titanic.txt", header = T)
titanic$Sex <- factor(titanic$Sex)
titanic$PClass <- factor(titanic$PClass)
titanic$Survived <- factor(titanic$Survived, levels=c(1,0), labels=c("Yes", "No"))
```

```{r}
survived = titanic$Survived
plot(survived ~ factor(titanic$PClass), main='Ratio of survivors per class', xlab='Passenger class', ylab='survived')
```

The graph shows that the higher the passenger class, the bigger
percentage or survivors.

```{r}
plot(survived, factor(titanic$PClass), main='Ration of survivors per class', ylab='Passenger class', xlab='survived')
```

Also the higher the passenger class, the more survivors.

```{r}
plot(factor(titanic$Sex), survived, main='Ratio of survivors per sex', xlab='sex', ylab='survived')
```

And that in total there are more female survivors then male survivors.

```{r}
par(mfrow=c(1,2))
hist(titanic$Age, breaks = seq(0, 80, 5), main='Age of all', xlab='Age')
plot(survived ~ titanic$Age, main='Ratio of survival per Age', xlab='age', ylab='survived')
par(mfrow=c(1,1))
```

The most people were between the ages of 15 and 30, but the age group
under 5 has the highest survival rate.

```{r results='hide'}
titanic = read.table("titanic.txt", header = T)
titanic$Sex <- factor(titanic$Sex, levels=c("male", "female"), labels=c(1, 0))
titanic$PClass <- factor(titanic$PClass,levels=c("1st", "2nd", "3rd"), labels=c(1, 2, 3))
titanic$Survived <- factor(titanic$Survived)
model <- glm(Survived ~ PClass + Age + Sex, data=titanic, family=binomial(link="logit"))
summary(model)
```

```{r}
odds_ratio <- exp(coef(model))
odds_ratio <- data.frame(feature=names(odds_ratio), odds_ratio=odds_ratio)
odds_ratio
```

The odds ratios for PClass2 and PClass3 are calculated based on the
reference category of PClass1. The odds ratio for PClass2 of 0.275 means
that the odds of survival for a passenger in second class are 0.275
times the odds of survival for a passenger in the reference category of
PClass1. This suggests that passengers in second class were less likely
to survive than passengers in first class, holding the other variables
constant. Similarly, the odds ratio for PClass3 of 0.080 means that the
odds of survival for a passenger in third class are 0.080 times the odds
of survival for a passenger in the reference category of PClass1 (first
class), holding the other variables constant. The odds ratio for Age is
0.965, which means that for each one-year increase in age, the odds of
survival decrease by a factor of about 0.965, holding the other
variables constant. The odds ratio for Sex0 is 13.8926071, which means
that the odds of survival for a female passenger are about 13.8926071
times the odds of survival for a male passenger, holding the other
variables constant. This suggests that female passengers were more
likely to survive than male passengers.

Overall, the results suggest that female passengers and those in higher
classes (i.e. first and second class) were more likely to survive than
male passengers and those in third class. Age was also a significant
predictor, with older passengers being less likely to survive than
younger passengers.

### b)

<!--# Investigate the interaction of predictor Age with PClass, and the interaction of Age with Sex. From this and a), choose (and justify) a resulting model. For this model, report the estimate for the probability of survival for each combination of levels of the factors PClass and Sex for a person of age 55.-->

```{r}
model <- glm(Survived ~ PClass + Age + Sex + Age:PClass + Age:Sex, data=titanic, family=binomial(link="logit"))
summary(model)
```

To choose the resulting model, we need to evaluate the statistical
significance of the interaction terms. If they are not significant, we
can stick with the original model without the interaction terms. In the
summary output, the p-values for the interaction terms PClass2:Age and
Age:Sex0 are 0.04012 and 1.52e-06, respectively. Both of these p-values
are less than 0.05, which suggests that the interactions are
significant. Therefore, it is appropriate to include the interaction
terms in the model.

Once we have our final model, we can use it to estimate the probability
of survival for each combination of levels of the factors PClass and Sex
for a person of age 55.

```{r}
newdata <- expand.grid(PClass=levels(titanic$PClass), Sex=levels(titanic$Sex))
newdata$Age <- rep(55, nrow(newdata))

# use the predict() function to get the predicted probabilities for each combination of PClass, Sex, and Age
newdata$SurvivalProb <- predict(model, newdata, type="response")

newdata
```

As we can see, indeed the highest probability of the survival denotes to
the female passenger of the 1st class, while the lowest - to male
passenger of the 3rd class.

### c)

<!--# c) Propose a method to predict the survival status and a quality measure for your prediction and describe how you would implement that method (you do not need to implement it). -->

One method to predict the survival status is to use the final model
**tlm** and apply it to new data containing the predictor variables for
each passenger. Moreover, we can divide the dataset into training and
testing sets. The training set is then used to train a logistic
regression model, which is then used to predict the survival status of
the testing set. We compute metrics such as accuracy and precision to
evaluate the model's performance. This process is repeated until we find
the best-performing model, which we can use to predict the survival
status of new passengers based on their characteristics.

### d)

<!--# Another approach would be to apply a contingency table test and to investigate whether factor passenger class has an effect on the survival status and whether factor gender has an effect on the survival status. Implement the relevant test(s). -->

```{r}
titanic = read.table("titanic.txt", header = T)
PClass = factor(titanic$PClass)
Survived = factor(titanic$Survived)
t = table(PClass, Survived); t

z = chisq.test(t); z
residuals(z)
```

The p-value tells that there is a significant association between the
passenger class and the survival status. According to the table,
first-class passengers are the most likely to survive, while third-class
passengers are the least likely to survive.

```{r}
Sex = factor(titanic$Sex);
t = table(Sex, Survived); t
z = chisq.test(t); z
residuals(z)
```

The p-value tells that there is a significant effect of gender factor on
the survival status. According to the table, female are relatively more
likely to survive.

### e)

<!--# Is the second approach in d) wrong? Name both an advantage and a disadvantage of the two approaches, relative to each other. -->

## Exercise 4

### a)

<!--# Perform Poisson regression on the full data set, taking miltcoup as response variable. Comment on your findings. -->

```{r}
coups = read.table("coups.txt", header = T)
data <- coups

# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numelec + numregim, data = data, family = "poisson")

# summarize the model
summary(model)

```

The output of this code will provide us with a summary of the Poisson
regression model.

We can observe that the oligarchy variable has a positive coefficient
estimate of 0.0731, which means that an increase in the number of years
a country has been ruled by a military oligarchy is associated with an
increase in the number of successful military coups. This variable is
statistically significant at the 0.05 level, as the p-value is 0.0347.

The pollib variable has a negative coefficient estimate of -0.713, which
means that an increase in political liberalization is associated with a
decrease in the number of successful military coups. This variable is
also statistically significant as the p-value is 0.0089.

The parties variable has a positive coefficient estimate of 0.0308,
which means that an increase in the number of legal political parties is
associated with an increase in the number of successful military coups.
This variable is also statistically significant as the p-value is
0.00595.

The other variables do not have a statistically significant effect on
the number of successful military coups.

### b) 

<!--# Use the step-down approach (using output of the function summary) to reduce the number of explanatory variables. Compare the resulting model with your findings in a). -->

To perform the step-down approach for variable selection, we start with
the full model and iteratively remove variables until all remaining
variables are significant at the 0.05 level. We have the full model in
a), we can see that the numelec variable has the biggest p-value, so it
can be a decent starting point for the removal.

```{r}
# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size  + numregim, data = data, family = "poisson")

# summarize the model
summary(model)
```

Next, we see that the numregim variable is not significant, and remove
it.

```{r}
# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size, data = data, family = "poisson")

# summarize the model
summary(model)
```

The next candidate for the elimination is size variable.

```{r}
# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn, data = data, family = "poisson")

# summarize the model
summary(model)
```

After removing the popn variable we see the following results.

```{r}
# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties + pctvote, data = data, family = "poisson")

# summarize the model
summary(model)
```

After removing pctvote variable:

```{r}
# fit the Poisson regression model
model <- glm(miltcoup ~ oligarchy + pollib + parties, data = data, family = "poisson")

# summarize the model
summary(model)
```

After executing the step-down approach we can see that the variables
remain the same, however, their p-values decreased even further. Unlike
initial model, in the new one the oligarhy variable has the most
significant impact on the miltcoup.

The initial model has a lower residual deviance and AIC compared to the
model after the step-down approach, indicating that the initial model
fits the data slightly better. However, the model after the step-down
approach has a simpler structure with fewer explanatory variables, which
can be more desirable in certain situations, such as when interpreting
the model and making predictions.

Overall, the step-down approach has reduced the number of explanatory
variables and provided a simpler model with comparable findings to the
initial model. However, depending on the research question and context,
one may choose to use the initial model for its better fit or the
simplified model for its simplicity.

### c)

<!--# Using the model from b), predict the number of coups for a hypothetical country for all the three levels of political liberalization and the (overall) averages of all the other (numerical) characteristics. Comment on your findings. -->

```{r}
# set overall averages of all the other numerical characteristics
newdata <- data.frame(
  oligarchy = mean(data$oligarchy),
  pollib = mean(data$pollib),
  parties = mean(data$parties),
  pctvote = mean(data$pctvote),
  popn = mean(data$popn),
  size = mean(data$size),
  numelec = mean(data$numelec),
  numregim = mean(data$numregim)
)

round(newdata, digits = 0)
newdata

newdata$pollib <- 0
predict(model, newdata, type = "response")

newdata$pollib <- 1
predict(model, newdata, type = "response")

newdata$pollib <- 2
predict(model, newdata, type = "response")

```

From the results we can observe that a country with no civil rights for
political expression (pollib = 0) is predicted to have the highest
number of military coups (3.040149) compared to a country with limited
civil rights for expression but right to form political parties (pollib
= 2: 1.712241) and a country with full civil rights (pollib = 2:
0.9643509) when other variables are held at their average levels. This
finding seems intuitive as one might expect more political freedom to
result in fewer military coups. As a point of discussion we can mention
the fact that the model was fitted using data up until 1989, so it may
not be applicable to more recent years.
