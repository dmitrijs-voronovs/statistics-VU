---
title: "Assignment 1"
author: "Group 69: Dmitrijs Voronovs (2779206), Nikita ..., Alina ..."
date: "24.02.2023"
output: 
  pdf_document: 
    latex_engine: lualatex
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

## Exercise 1

```{=html}
<!--# 
The Amsterdamsche Bos forestry wishes to estimate the total wood volume of the trees on its domain. To this end the forestry has cut a sample of 59 trees of their most prevalent types beech and oak and collected the data from the cut trees in the file treeVolume.txt Download treeVolume.txt. The volume of these trees alongside with their height and trunk diameter have been measured; these are the columns volume, height and diameter, respectively. Column type gives the tree type: Beech or Oak. The tree type, height and diameter can be measured in the field without sacrificing the tree. The forestry hypothesizes that these are predictive of the wood volume.
 -->
```
### a)

```{=html}
<!--# 
a)  Investigate whether the tree type influences volume by performing ANOVA, without taking diameter and height into account. Can a t-test be related to the above ANOVA test? Estimate the volumes for the two tree types.
 -->
```
```{r echo = T, results = 'hide'}
treeVolume = read.table("treeVolume.txt", header = T)
head(treeVolume)
```

```{r echo = F, results = 'hide'}
volumelm = lm(volume ~ factor(type), data = treeVolume)
anova(volumelm)
```

Since P value for type is 0.1736, it indicates that we fail to reject
the null hypothesis and conclude that there is no significant difference
in mean volume between the beech and oak trees.

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(volumelm))
plot(fitted(volumelm), residuals(volumelm))
par(mfrow=c(1,1))
```

After testing the data for normality, it becomes obvious that there is a
deviation of residuals from the normal distribution. As ANOVA
assumptions have been violated, p-value may not be reliable.

```{r echo = T, results = 'hide'}
t.test(volume ~ factor(type), data=treeVolume)
```

T-test shows the p-value 0.1659, indicating that we do not reject a null
hypothesis. That indicated the difference in means between volumes of
beech and oak is different.

T-test also displays estimates of mean for group beech (30.17097) and
oak (35.25000)

### b)

```{=html}
<!--# 
b)  Now include diameter and height as explanatory variables into the analysis. Investigate whether the influence of diameter on volume is similar for the both tree types. Do the same for the influence of height on volume. (Consider at most one (relevant) pairwise interaction per model.) Comment.
 -->
```
```{r results='hide'}
volumeFulllm1 = lm(volume ~ factor(height) + factor(type) * factor(diameter), data = treeVolume)
anova(volumeFulllm1)
```

<!--#TODO: clarify that summary shows only factor(type)oak.Should we use separate data for each model i.e. oak_data <- subset(treeVolume, type == "oak")-->

```{r}
volumeFulllm2 = lm(volume ~ factor(diameter) + factor(type) * factor(height), data = treeVolume)
anova(volumeFulllm2)
```

### c)

```{=html}
<!--# 

c)  Using the results from c), investigate how diameter, height and type influence volume. Comment. Using the resulting model, predict the volume for a tree with the (overall) average diameter and height?

 -->
```
```{r}
volumeFulllm = lm(volume ~ factor(diameter) + factor(height) + factor(type), data = treeVolume)
anova(volumeFulllm)
```

<!--# TODO: how to comment? -->

```{r}
# TODO: how to use mean here? We were training on factors
# d = mean(treeVolume$diameter)
# h = mean(treeVolume$height)
d = median(treeVolume$diameter)
h = median(treeVolume$height)
avgTree = data.frame(diameter=d, height=h, type="oak")
avgTrees = data.frame(diameter=c(d,d), height=c(h,h), type=c("oak", "beech"))

predict(volumeFulllm, avgTree, interval = 'prediction')
```

### d)

```{=html}
<!--# 

d)  Propose a transformation of the explanatory variables that possibly yields a better model (verify this).  (Hint: think of a natural link between the response and explanatory variables.)

 -->
```
The least significant factor can be removed from the explanation, which
is type, as when calculating the volume only height and diameter should
be of a significance. However, while type by itself does nothing, it
explains the height or the diameter, therefore its interaction with one
of other factors should be included.

```{r}
volumeOptimallm = lm(volume ~ factor(diameter) + factor(height) + factor(height):factor(type), data = treeVolume)
anova(volumeOptimallm)
```

<!--# TODO: is it good enough? How to measure the model effectivnesS? lecture 8, Diagnostic in linear regression?? -->

## Exercise 2

<!--# 

The data in expensescrime.txt Download expensescrime.txt were obtained to determine factors related to state expenditures on criminal activities (courts, police, etc.) The variables are: state (indicating the state in the USA), expend (state expenditures on criminal activities in $1000), bad (crime rate per 100000), crime (number of persons under criminal supervision), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in 1000). In the regression analysis, take expend as response variable and bad, crime, lawyers, employ and pop as explanatory variables.

 -->

### a)

<!--# 

a)  Make some graphical summaries of the data. Investigate the problem of influence points, and the problem of collinearity.

 -->

```{r}
expensescrime = read.table("expensescrime.txt", header = T)
pairs(expensescrime[,-1])
```

Influence points:

many paired scatter plots (i.e. expend vs bad, layers vs employ) have
most of the data is skewed to the left and there are strong outliers on
the right.

<!--# TODO: add plots, etc. -->

Collinearity:

Multiple variables are clearly collinear: bad & employ
(`cor(expensescrime$bad, expensescrime$employ)` is 0.871), bad & lawyers
(0.832), bad & pop (0.92), lawyers & employ (0.966), lawyers & pop
(0.934)

### b)

<!--# b)  Fit a linear regression model to the data. Use the step-up method to find the best model. Comment. -->

<!--# TODO: Cleanup afterwards -->

```{r}
summary(lm(expend ~ factor(bad), data=expensescrime))$r.squared
summary(lm(expend ~ bad, data=expensescrime))$r.squared
summary(lm(expend ~ crime, data=expensescrime))$r.squared
summary(lm(expend ~ lawyers, data=expensescrime))$r.squared
summary(lm(expend ~ employ, data=expensescrime))$r.squared
summary(lm(expend ~ pop, data=expensescrime))$r.squared


```

<!--# TODO: should we use factor(bad) only or with others as well? Should we include it??? It always gives 1 -->

The first variable to add is **employ**, as it is significant and yields
the best multiple R-squared value **0.9539745**

```{r}
summary(lm(expend ~ employ + bad, data=expensescrime))$r.squared
summary(lm(expend ~ employ + crime, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers, data=expensescrime))$r.squared
summary(lm(expend ~ employ + pop, data=expensescrime))$r.squared
```

Next one to add is **lawyers** with the R-squared value **0.9631745**

```{r}
summary(lm(expend ~ employ + lawyers + bad, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers + crime, data=expensescrime))$r.squared
summary(lm(expend ~ employ + lawyers + pop, data=expensescrime))$r.squared
```

Other variables upon testing showed no significance, therefore the final
model is

```{r}
model = lm(expend ~ employ + lawyers, data=expensescrime)
```

### c)

<!--# c)  Determine a 95% prediction interval for the expend using the model you preferred in b) for a (hypothetical) state with bad=50, crime=5000, lawyers=5000, employ=5000 and pop=5000. Can you improve this interval? -->

```{r}
newData = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(model, newData, interval = 'prediction')
```

Interval may be improved by adding interaction between variables. That
could possible result in a better fit, thus precise interval and a
predication.

### d)

## Exercise 3

### a)

<!--# a)  Study the data and give a few (>1) summaries (graphics or tables). Fit a logistic regression model (no interactions yet) to investigate the association between the survival status and the predictors PClass, Age and Sex. Interpret the results in terms of odds, comment. -->

```{r}
titanic = read.table("titanic.txt", header = T)
```

```{r}
titlm = lm(Survived ~ factor(PClass) + Age + factor(Sex), data=titanic)
summary(titlm)
```

<!--# TODO: Why is that? -->

All the predictor variables show significance. Female sex is not
represented in the summary

### b)

<!--# b)  Investigate the interaction of predictor Age with PClass, and the interaction of Age with Sex. From this and a), choose (and justify) a resulting model. For this model, report the estimate for the probability of survival for each combination of levels of the factors PClass and Sex for a person of age 55. -->

```{r}
titIntlm1 = lm(Survived ~ factor(PClass) + Age + factor(Sex) + factor(PClass):Age, data=titanic)
summary(titIntlm1)

titIntlm2 = lm(Survived ~ factor(PClass) + Age + factor(Sex) + factor(Sex):Age, data=titanic)
summary(titIntlm2)

titIntlm3 = lm(Survived ~ factor(PClass) + Age + factor(Sex) + factor(PClass):Age + factor(Sex):Age, data=titanic)
summary(titIntlm3)
```

<!--# TODO: how to interpret? Look at RSE only and p-values for interactions? -->

<!--# TODO: when testing interactions, are there any benefits in compering them separately or including all of them in a single model and comparing in a final summary? -->

*PClass : age* interaction is not significant, thus it should not be
included in the final model, while *age : sex* interaction is shown as
significant only for male. The proposal for the final model is as
follows

```{r}
tlm = lm(Survived ~ factor(PClass) + Age + factor(Sex) + factor(Sex):Age, data=titanic)
summary(tlm)

tlm = lm(Survived ~ factor(PClass) + factor(Sex) + factor(Sex):Age, data=titanic)
summary(tlm)
```

<!--# TODO: is that true? Why no RSE change? -->

Age is not significant by itself anymore, thus it can be removed.

### c)

<!--# c) Propose a method to predict the survival status and a quality measure for your prediction and describe how you would implement that method (you do not need to implement it). -->

<!--# Refine: -->

One method to predict the survival status is to use the final model tlm
and apply it to new data containing the predictor variables for each
passenger. The resulting model can be used to predict the probability of
survival for each passenger.

To measure the quality of the prediction, we can use metrics such as
accuracy, precision, recall, and F1-score. These metrics compare the
predicted survival status to the actual survival status for each
passenger. Accuracy measures the overall proportion of correct
predictions, while precision measures the proportion of true positives
among all positive predictions and recall measures the proportion of
true positives among all actual positives. The F1-score is the harmonic
mean of precision and recall and provides a balanced measure of the
model's performance.

To implement this method, we would first split the titanic dataset into
a training set and a test set. We would then fit the final model tlm to
the training set and use it to predict the survival status for each
passenger in the test set. We would calculate the quality measures for
the predictions and use them to evaluate the performance of the model.
If the performance is satisfactory, we can use the model to predict the
survival status for new passengers in the future.

### d)

### e)

## Exercise 4

### a)

<!--# a)  Perform Poisson regression on the full data set, taking miltcoup as response variable. Comment on your findings. -->

```{r}
coups = read.table("coups.txt", header = T)
```

```{r}
model <- glm(miltcoup ~ ., data = coups, family = "poisson")
summary(model)
```

### b)

### c)
